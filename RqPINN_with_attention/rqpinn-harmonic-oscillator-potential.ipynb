{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the problem defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "-\\frac{\\hbar^2}{2m} \\frac{d^2  \\psi}{dx^2}  + V(x) \\psi =  E \\psi\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "*   $\\hbar $ is the reduced Planck's constant\n",
    "*   $m$ is the mass of particle\n",
    "*  $\\psi $ is the wavefunction (representing the eigenfunction/ eigenstate)\n",
    "* $V(x) $ is the potential energy of the particle at any point $x$,\n",
    "defined as\n",
    "\\begin{equation}\n",
    "    V(x) = \\frac{1}{2}m\\omega^2x^2\n",
    "\\end{equation}\n",
    "* $E$ is the energy of the particle (representing the eigenvalue of the equation)\n",
    "\n",
    "Taking $l = 10$, $\\hbar = 1, m = 1 $ and $\\omega = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim     #optimizer\n",
    "import time\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "#Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "\n",
    "#!pip install pyDOE\n",
    "from pyDOE import lhs #latin hypercube sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sine activation function\n",
    "class sine(torch.nn.Module):\n",
    "  @staticmethod\n",
    "  def forward(input):\n",
    "    return torch.sin(input)\n",
    "\n",
    "#Automatic differentiation in pytorch\n",
    "def dfx(f, x):\n",
    "  grads = []\n",
    "  for i in range(f.shape[1]):  # Loop over each column\n",
    "      gouts = torch.zeros_like(f)\n",
    "      gouts[:, i] = 1.0  # Differentiate w.r.t. one column at a time\n",
    "      df_i = grad([f], [x], grad_outputs=gouts, create_graph=True)[0]\n",
    "      grads.append(df_i)  # Collect gradients for each column\n",
    "\n",
    "  return torch.stack(grads, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameters\n",
    "t0 =-5; tf = 5; xbc = 0; eig_num = 5; gauss_num = 10 #Guess_num = 20 for the other pre_trainer\n",
    "col_pts = 50 ;  layers = [1,512,512,256,256,128,gauss_num] #This is layer structure for the pretrained model\n",
    "epochs =10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Parametric Solution\n",
    "#This enforces the solution on the boundaries\n",
    "def parametricSolutions(t,nn,t0,tf, bound):\n",
    "  N1 = nn(t)\n",
    "  f = ((1-torch.exp(-(t - t0)))*(1-torch.exp(t - tf))).view(-1,1)\n",
    "  psi_hat = bound + f*N1 #Broadcasting is done here\n",
    "  return psi_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamEqs_Loss(t,psi,v):\n",
    "    psi_x = dfx(psi,t).squeeze(1)\n",
    "    psi_xx = dfx(psi_x,t).squeeze(1)\n",
    "    E  = torch.sum(psi* (-0.5*psi_xx + psi*v), dim = 0)/torch.sum(psi * psi, dim = 0) #The eigenvalues\n",
    "    L = (psi_xx) + (E-v)*psi\n",
    "    return L,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the potential well\n",
    "def V(x):\n",
    "    y = 0.5*x**2\n",
    "    return y\n",
    "x = torch.linspace(t0,tf,10).view(-1,1)\n",
    "pot = V(x)\n",
    "#scaled_pot = V_norm(x)\n",
    "\n",
    "plt.plot(x,pot, label='V')\n",
    "#plt.plot(x,scaled_pot, label='V_norm')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Frobenius norm\n",
    "def frobenius_norm(G):\n",
    "  I = torch.eye(G.shape[0], device = device, dtype = G.dtype)\n",
    "  loss = torch.linalg.matrix_norm(G - I) #The frobenius norm\n",
    "  return loss\n",
    "\n",
    "#Defining the monotonicity_penalty\n",
    "def monotonicity_penalty(E, margin = 0.0):\n",
    "  diffs = E[:-1] - E[1:] + margin\n",
    "  penalty = torch.relu(diffs).sum()\n",
    "  return penalty\n",
    "\n",
    "#Defining the cosine matrix using trapezoidal rule\n",
    "def cosine_matrix(psi,x):\n",
    "  x = x.squeeze(-1) #Remove the extra dimension\n",
    "  x_sorted, indices = torch.sort(x, dim=0)\n",
    "  psi_sorted = psi[indices, :]  # Reorder psi to match x\n",
    "  psi_norm = torch.sqrt(torch.trapz(psi_sorted**2, x_sorted, dim=0))  # (k,)\n",
    "  psi_normalized = psi_sorted / psi_norm  # (n, k)\n",
    "  psi_prod = psi_normalized.unsqueeze(2) * psi_normalized.unsqueeze(1)  # (n, k, k)\n",
    "  cos_matrix = torch.trapz(psi_prod, x_sorted, dim=0)\n",
    "  return cos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pre_trainerclass(torch.nn.Module):\n",
    "  def __init__(self,layers):\n",
    "    super(pre_trainerclass,self).__init__()\n",
    "    self.activation = sine()\n",
    "\n",
    "    #Defining the Layers\n",
    "    self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers) - 1)])\n",
    "\n",
    "    #Initializing the weights\n",
    "    #self.apply(weights_init)\n",
    "    for i in range(len(layers)-1):\n",
    "      nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "      nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "\n",
    "  #Defining the forward pass\n",
    "  def forward(self,x):\n",
    "    if torch.is_tensor(x) != True:\n",
    "      x = torch.from_numpy(x)\n",
    "    a = x.float()\n",
    "    #In1 = self.ein(torch.ones_like(a))\n",
    "    for i in range(len(self.linears)-1):\n",
    "      z = self.linears[i](a)\n",
    "      #z = self.batch_norms[i](z)\n",
    "      a = self.activation(z)\n",
    "\n",
    "    a = self.linears[-1](a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze for feature extraction, or fine-tune\n",
    "class transferred_model(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.activation = sine()\n",
    "        self.features_list = nn.ModuleList(pretrained_model.linears)  # Include all layers\n",
    "        self.new_head = nn.Linear(10, eig_num)\n",
    "\n",
    "        #Freeze pretrained model weights\n",
    "        for param in self.features_list.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, t):\n",
    "        for layer in self.features_list:\n",
    "            t = self.activation(layer(t))\n",
    "        return self.new_head(self.activation(t))\n",
    "    \n",
    "    def unfreeze_last_n_layers(self, n):\n",
    "        \"\"\"\n",
    "        Unfreezes the last `n` layers of the pretrained feature extractor.\n",
    "        \"\"\"\n",
    "        n = min(n, len(self.features_list))\n",
    "        for i in range(1,n+1):\n",
    "            for param in self.features_list[-i].parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we need to load the trained model weights for transfer learning then train for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_model = pre_trainerclass(layers) #holder to initialize the pre-trained model weights\n",
    "\n",
    "#The checkpoint\n",
    "checkpoint = torch.load('checkpoints/gauss_modulated_pre_trainer.pth', map_location=torch.device('cuda'))\n",
    "\n",
    "gaussian_model.load_state_dict(checkpoint['model_state_dict'])  #Load the weights into the holder\n",
    "#gaussian_model.eval() #Set to evaluation\n",
    "transfer_model = transferred_model(gaussian_model) #Transfer the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Finder\n",
    "We will find the best learning rate to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to find the best lr for our defined model\n",
    "model_lr = transfer_model.to(device)\n",
    "optimizer_lr = optim.Adam(model_lr.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_train_step(NN,t0,tf,xbc,t_rand):\n",
    "    vx = V(t_rand)\n",
    "    #Getting the parametric solution\n",
    "    psi =parametricSolutions(t_rand,NN,t0,tf,xbc).to(device) #The forward pass\n",
    "    L,E = hamEqs_Loss(t_rand,psi,vx)\n",
    "    Loss = torch.nn.MSELoss()\n",
    "    #Getting the losses after each forward pass\n",
    "    #We can also add the monotonicty loss\n",
    "    pde_loss = Loss(L,torch.zeros_like(L))  #The scaled pde_loss\n",
    "    G = cosine_matrix(psi, t_rand)\n",
    "    cosine_loss = frobenius_norm(G)\n",
    "    return pde_loss, cosine_loss, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables\n",
    "x_lhs = torch.tensor(t0 + (tf - t0)*lhs(1,col_pts)).view(-1,1).to(device).float()\n",
    "x_lhs.requires_grad =True\n",
    "losses = []\n",
    "lrs = []\n",
    "\n",
    "start_lr = 1e-7\n",
    "end_lr = 1e-1\n",
    "num_iter = 100\n",
    "lr_mult = (end_lr / start_lr) ** (1/num_iter)\n",
    "\n",
    "lr = start_lr\n",
    "\n",
    "for iteration in range(num_iter):\n",
    "    optimizer_lr.param_groups[0]['lr'] = lr\n",
    "\n",
    "    optimizer_lr.zero_grad()\n",
    "    pde_loss,cosine_loss, _ = common_train_step(model_lr,t0,tf,xbc,x_lhs)\n",
    "    loss = pde_loss + cosine_loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer_lr.step()\n",
    "\n",
    "    lrs.append(lr)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    lr *= lr_mult  # increase learning rate exponentially\n",
    "\n",
    "# Plot the result\n",
    "plt.plot(lrs, losses)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('LR Range Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the lr finder plot will change with different weights for the losses, so we should update accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we initialize the model without freezing the pretrained weights\n",
    "transfer_model.to(device)\n",
    "print(transfer_model)\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr=1e-5)\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(NN,t0,tf,xbc,epochs,t):\n",
    "  En_history = [] #To store the eigenvalues as it is optimized\n",
    "  loss_keys = ['pde_loss_hist','cos_loss_hist','total_loss_hist']\n",
    "  loss_dic = {key: [] for key in loss_keys} #To store the losses\n",
    "  #t_rand = t[torch.randperm(t.shape[0])].to(device).float() #Reshuffling the collocation points\n",
    "  t_rand = t.to(device).float()\n",
    "  t_rand.requires_grad =True\n",
    "\n",
    "  #Dictionary to store the final training\n",
    "  dic = {}\n",
    "\n",
    "  stime = time.time()\n",
    "  for iter in range(epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "    criteria_loss,cosine_loss,En = common_train_step(NN,t0,tf,xbc,t_rand)\n",
    "    total_loss = criteria_loss + cosine_loss\n",
    "    \n",
    "    En_history.append(En.cpu().detach().numpy())\n",
    "    \n",
    "    loss_dic['pde_loss_hist'].append(criteria_loss.cpu().detach().numpy().item())\n",
    "\n",
    "    loss_dic['cos_loss_hist'].append(cosine_loss.cpu().detach().numpy().item())\n",
    "\n",
    "    if iter == 500:\n",
    "      transfer_model.unfreeze_last_n_layers(2)\n",
    "\n",
    "\n",
    "    if iter % 200 == 0:\n",
    "      print('--------------------------------------------')\n",
    "      print(f'Epoch: {iter}')\n",
    "      print(f'En: {En_history[-1]}')\n",
    "      print('--------------------------------------------')\n",
    "      print('Total_loss: ',total_loss.item())\n",
    "      print('pde_loss: ',criteria_loss.item())\n",
    "      print('cos_loss: ',cosine_loss.item())\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "    #Backward propagation\n",
    "    total_loss.backward()\n",
    "    #Introducing Gradient clipping\n",
    "    #max_norm =12.0\n",
    "    #torch.nn.utils.clip_grad_norm_(NN.parameters(), max_norm)\n",
    "    optimizer.step()\n",
    "    #scheduler.step()\n",
    "\n",
    "    #Storing the loss after each iteration\n",
    "\n",
    "    loss_dic['total_loss_hist'].append(total_loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "  #Storing the final eigenfunction and eigenvalue\n",
    "  dic[1] = copy.deepcopy(NN)\n",
    "\n",
    "\n",
    "  ftime = time.time()\n",
    "  ttime = (ftime - stime)/60\n",
    "  print(f'Total training time is {ttime} minutes')\n",
    "  return loss_dic, np.array(En_history), dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lhs = torch.tensor(t0 + (tf - t0)*lhs(1,col_pts)).view(-1,1).float()\n",
    "x_lhs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss, eig_vals, qPINN = nn_train(transfer_model,t0,tf,xbc,epochs, x_lhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tTest = torch.linspace(t0,tf,1200).view(-1,1)\n",
    "tTest.requires_grad=True\n",
    "t_net = tTest.detach().numpy()\n",
    "\n",
    "psi = parametricSolutions(tTest,qPINN[1].cpu(),t0,tf,xbc).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the number of rows and columns for the subplot grid\n",
    "num_rows = 2\n",
    "num_cols = 3\n",
    "\n",
    "# Calculating the total number of subplots needed\n",
    "total_subplots = eig_vals[-1].shape[0]\n",
    "\n",
    "# Creating a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 12))\n",
    "\n",
    "# Iterating over eigenvalues and corresponding solutions\n",
    "for index, eig in enumerate(eig_vals[-1,:total_subplots]):\n",
    "    row_index = index // num_cols  # row index\n",
    "    col_index = index % num_cols   # column index\n",
    "\n",
    "    # Plot on the corresponding subplot\n",
    "    #psi_i = psi[index] + En_counter[index]\n",
    "    ax = axes[row_index, col_index]\n",
    "    max_val = abs(max(psi[:,index]))\n",
    "    ax.plot(t_net, psi[:,index]/max_val, color='blue', linewidth=1, label=f'PINN with eigenvalue {eig_vals[-1,index]:.5f}')\n",
    "    #ax.plot(t_net, V(torch.tensor(t_net)))\n",
    "    #ax.plot(t_net, np.zeros(len(t_net)),'--k', linewidth=1)\n",
    "    ax.set_xlabel('x', color='black')\n",
    "    ax.set_ylabel('$\\psi(x)$', color='black')\n",
    "    ax.tick_params(axis='y', color='black')\n",
    "    ax.legend(loc = 'upper left')\n",
    "\n",
    "# Hiding the unused subplot\n",
    "if total_subplots < num_rows * num_cols:\n",
    "    for i in range(total_subplots, num_rows * num_cols):\n",
    "        fig.delaxes(axes.flat[i])\n",
    "# Adjusting layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = psi.T @ psi\n",
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfering with freezing pre-trainer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
